In this project we will study methods for searching and exploring large amounts of data in modern information-access systems. Our aim is to go beyond identifying relevant content for individuals, and striving to maximize the diversity and fairness of the information presented to them. The indented use of these methods is to mitigate biases, promote a well-rounded understanding of topics, and avoid blind spots created by existing methods for filtering and prioritizing online content.

On the other hand, it is not uncommon that methods that have been designed for potentially beneficial use have unintentional and harmful consequences. As a concrete example, it could happen that, in an effort to present diverse and fair information to individuals, our methods will select content that falls into the fringes of what is considered socially acceptable, contain radical ideas, or untruths. In this particular example, however, we will claim that the unintentional abuse falls beyond the scope of our approach, since the presumed malign content is already present in the system, and individuals can reach to it via different routes. In other words, we believe that the unintentional abuse of our methods can be prevented by incorporating standard safeguard mechanisms to ensure the safety and legality of the available data, in addition to rigorous impact assessments and audits before and during deployment. Thus, we are confident that overall, our methods can contribute to the health of information-access systems.

Discussions and reflections on the ethical considerations for our methods will be held regularly with the project team members. We will also promote the culture of discussing ethical considerations in the reflection section of our research papers.